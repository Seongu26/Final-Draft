---
title: "FinalProject131"
author: "Seongu Lee"
date: "5/31/2022"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages("vembedr")
library(xgboost)
library(dplyr)
library(tidymodels)
library(ISLR)
library(tidyverse)
library(ggplot2)
library(corrplot)
library(glmnet)
library(janitor)
library(rpart.plot)
library(ranger)
library(vip)
library(reshape2)
library("vembedr")
```

### Introduction

This project is about finding a best model to predict the diagnosis of diabetes with given data set. 

### Diabetes In Real Life

```{r}
embed_youtube("wZAjVQWbMlE")
```

Diabetes is a disease that occurs when our blood glucose is very high (126mg/dL). There are two types of diabetes. If we have type 1 diabetes, bodies don`t make insulin that helps glucose from food get into your cells to be used for energy. If we have type 2 diabetes, bodies don`t make or use insulin well. Okay. Now we know what Diabetes is. But, is it  always true that one person will have diabetes if his/her test score is more than 126. The answer is NO. There will be more factors help people for diabetes diagnosis. From this project, I will find the variables that affect to diabetes. And I will predict diabetes diagnosis based on those variables with a model I picked at the end.


## Data loading

I will be using a data set that described 8 variables and outcome of diagnosis. There are 768 rows. The detail will be shown in codebook. 

```{r}
data <- read.csv("C:/Users/sungu/Desktop/diabetes.csv") # read data
summary(data) # summary of raw data
```

Now I will clean the data to look better and more useful

# Data Cleaning

```{r}
clean<- clean_names(data) # make the name simple
clean$outcome <- factor(clean$outcome)
cleanDia<- subset(clean, clean$outcome == 1) # Cleaned data with diabetes outcome
cleanNon<- subset(clean, clean$outcome == 0) # Cleaned data with non-diabetes outcome

a<-is.na(clean) # find the null data
sum(a) # sum of the null datas
```

There is no zero data in this data set. SO it is good to use.

Based on the summary of raw data set, the values of Insulin is larger than other variables. And DiabetesPedigreeFunction and pregnancies are smaller than others. 
So, those should be seperated when plot is created.


### Data split

Since this data set is not large, I picked 0.8 for split percentage.
The dimension looks good(same values). 614 rows for training. 154 for testing. I also used factor for outcome to show plots better.

```{r}
split <- initial_split(clean, strata = outcome, prop = 0.8)
train <- training(split)
test <- testing(split)
dim(train)
dim(test)

 
```

### EDA

I made my data set split. Now I need to figure the relations among variables.

First, let's see the corrplot for correlation.

```{r}
train %>%
  select(is.numeric) %>% 
  cor() %>% 
  corrplot(type = 'full', diag = FALSE, 
           method = 'number')

```

Pregnancy and age are correlated. Outcome and glucose, bmi look correlated as well.
Outcome and blood pressure, skin thickness don't look correlated. 


Now I will show the outcome vs variables with scatterplot

#### bmi

```{r}
train$outcome = factor(train$outcome) # I also used factor for outcome to show plots better.

plot(train$outcome, train$bmi, main="Scatterplot outcome vs bmi",
   xlab="Outcome(diabetes) ", ylab="bmi", col = 'blue')

bmi <- melt(train,id.vars='outcome', measure.vars=c('bmi'))
bmiPlot <- ggplot(bmi) +
      geom_jitter(aes(x=outcome, y=value,  color=variable))
bmiPlot
```

This shows that there is significant effect with bmi. So, I should include bmi to model

#### glucose

```{r}
plot(train$outcome, train$glucose, main="outcome vs glucose",
   xlab="Outcome(diabetes) ", ylab="glucose", col = 'red')

glu <- melt(train,id.vars='outcome', measure.vars=c('glucose'))
gluPlot <- ggplot(glu) +
      geom_jitter(aes(x=outcome, y=value,  color=variable))
gluPlot
```

Based on plots, high Glucose value seems like higher chance to get diabetes. Definitly, I will keep this variable for modeling

####blood pressure

```{r}
plot(train$outcome, train$blood_pressure, main="Scatterplot outcome vs blood pressure",
   xlab="Outcome(diabetes) ", ylab="blood_pressure", col = 'green')

bloPree <- melt(train,id.vars='outcome', measure.vars=c('blood_pressure'))
bloPrePlot <- ggplot(bloPree) +
      geom_jitter(aes(x=outcome, y=value,  color=variable))
bloPrePlot
```

Having high blood pressure doesn't look like having higher chance to get diabetes. So, I would not use this for modeling

#### insulin

```{r}
plot(train$outcome, train$insulin, main=" outcome vs insulin",
   xlab="Outcome(diabetes) ", ylab="insulin", col = 'black') 

ins <- melt(train,id.vars='outcome', measure.vars=c('insulin'))
inspl <- ggplot(ins) +
      geom_jitter(aes(x=outcome, y=value,  color=variable))
inspl
```

Having high insulin looks like having higher chance to get diabetes. I will use this data for model

#### pregnancies

```{r}
plot(train$outcome, train$insulin, main=" outcome vs pregnancies",
   xlab="Outcome(diabetes) ", ylab="pregnancies", col = 'red') 

smaller <- melt(train,id.vars='outcome', measure.vars=c('pregnancies'))
q <- ggplot(smaller) +
      geom_jitter(aes(x=outcome, y=value,  color=variable))
q
```

pregnancies doesn't affect to have diabetes in jitter plot , but It affects in box plot. I need to keep this data.


#### DPF
```{r}
plot(train$outcome, train$insulin, main=" outcome vs diabetes_pedigree_function",
   xlab="Outcome(diabetes) ", ylab="diabetes_pedigree_function", col = 'blue') 

func <- melt(train,id.vars='outcome', measure.vars=c('diabetes_pedigree_function'))
t <- ggplot(func) +
      geom_jitter(aes(x=outcome, y=value,  color=variable))
t


```

I can see that having high or low diabetes pedigree function doesn't affect the result of diabetes test in jitter plot. But, box plot shows having higher DPF is having higher chance to get diabetes. So I would keep this data.

#### Age

```{r}
plot(train$outcome, train$insulin, main=" outcome vs age",
   xlab="Outcome(diabetes) ", ylab="age", col = 'green') 

funct <- melt(train,id.vars='outcome', measure.vars=c('age'))
ta <- ggplot(funct) +
      geom_jitter(aes(x=outcome, y=value,  color=variable))
ta
```

Age doesn't affect to have diabetes in jitter plot. But it is opposite on box plot. So, I should keep this data.


```{r}
plot(train$outcome, train$insulin, main=" outcome vs skin_thickness",
   xlab="Outcome(diabetes) ", ylab="skin_thickness", col = 'red') 


funct <- melt(train,id.vars='outcome', measure.vars=c('skin_thickness'))
ta <- ggplot(funct) +
      geom_jitter(aes(x=outcome, y=value,  color=variable))
ta
```

Skin thickness affects to have diabetes. Having thick skin will increase a chance to have diabetes based on the plot. I should keep this
So, I will use glucose, skin_thickness, insulin ,bmi , DPF , age and pregnancies

### More analysis before model building 

```{r}
summary(cleanDia)
summary(cleanNon)
```

### Model Building


```{r}

#train$glucose = factor(train$glucose)
#train$blood_pressure  = factor(train$blood_pressure)
#train$insulin = factor(train$insulin)
#train$bmi  = factor(train$bmi)

```

#### Fold

```{r}
folds <- vfold_cv(data = train, v = 3,repeats = 3)
```

I used 3 folds and repeated 5 times. Since I has 614 rows of training data set, it is good to use 3 folds only.

#### Recipe 

```{r}
recipe <- recipe(outcome ~ glucose+ skin_thickness+ insulin +bmi +diabetes_pedigree_function  +age + pregnancies , data = train) %>% 
   
  step_dummy(all_nominal_predictors()) %>% 
# step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```

I used step_dummy to make dummy predictors and I normalized. 

### Model searching

This part is about fitting models and comparing.


#### Decision_tree

This model is best-performing pruned tree.I use rpart and classification. ANd I set up workflow with recipe and decision tree model. Tuning grid is created with cost_complexity. levels was 10 and range was -3 to -1 as I studied from lab. 




```{r}
tree <- decision_tree(cost_complexity = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tree_wk <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(tree)

girds <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)


```

I executed the model with folds I created. And autoplot to see the tune performance.

```{r}

tune_res <- tune_grid(tree_wk,
                      resamples = folds,
                      grid = girds,
                      metrics = metric_set(roc_auc))
autoplot(tune_res)
```

I collect the metrics of mean and error. The bet auc is 0.789


```{r}
best<- collect_metrics(tune_res) %>% 
        arrange(mean)
best
best_auc<- max(best$mean)
best_auc
```
#### Random Forest

I used ranger for engine and classification.  Also, the impurity will provide variable importance scores for this model, which gives some insight into which predictors drive model performance.

```{r}
rf <- rand_forest() %>%
  set_engine("ranger",importance = "impurity") %>%
  set_mode("classification")
```

I stored this model and my recipe in a workflow.

```{r}

rf_workflow <- workflow() %>% 
  add_model(rf %>% set_args(mtry = tune(), trees = tune(), min_n = tune())) %>% 
  add_recipe(recipe)

```

I set up a tuning grid with 8 levels which lead better outcome. And mtry, trees, and min_n for grid.

```{r}
rf_grid <- grid_regular(mtry(range = c(1, 7)), trees(range = c(2,200)), min_n(range = c(1, 20)), levels = 7)
rf_grid
```

I executed my model by tuning and fitting and I saved it and autoplot it.

```{r}

rf_tune <- tune_grid( 
  rf_workflow, 
  resamples = folds, 
  grid = rf_grid, 
  metrics = metric_set(roc_auc) 
  )
  
```

```{r}

autoplot(rf_tune)
```

The auc was 0.835

```{r}
random <- collect_metrics(rf_tune) %>% 
          arrange(mean) 
random
random_auc<- max(tail(random$mean))
random_auc
```


#### Boosted tree

I will use boosted tree this time.

I had xgboost and classification for this model. And I stored the model and recipe in workflow.


```{r}
boost = boost_tree(trees = tune()) %>%
  set_engine("xgboost") %>% 
  set_mode("classification") 

boost_wk = workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(boost)
```

This is a set up for grid. And I use level 7  and range was 10 to 2000

```{r}
boost_grid <- grid_regular(trees(range = c(10,2000)), levels = 7)
```

I executed this model and autoplot it. After 350 of trees, the roc_auc dropped to 0.78.

```{r}
boost_tune <- tune_grid( 
  boost_wk, 
  resamples = folds, 
  grid = boost_grid, 
  metrics = metric_set(roc_auc) 
  )

autoplot(boost_tune) 
```

I will collect the metrics for the boosted tree tune. The mean is 0.81

```{r}
boostM<- collect_metrics(boost_tune) %>% 
          arrange(mean)
boostM
boost_auc<- max(boostM$mean)
boost_auc  
```

#### Multinomial logistic regression




Finally, I set up a Multinomial logistic regression. I used classification and glmnet for set model and engine. I set up a workflow . Also I got the tuning grid with penalty of range -5,5 and mixture of range 0,1 with levels 10. I also executed the model.
```{r}
multLog <- multinom_reg(penalty = tune(),  mixture = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

en_workflow <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(multLog)

en_grid <- grid_regular(penalty(range = c(-5, 5)), 
                        mixture(range = c(0, 1)), levels = 10)

tune_res <- tune_grid(
  en_workflow,
  resamples = folds, 
  grid = en_grid
)

```

I autoplot the tune model.

```{r}
autoplot(tune_res)
```

```{r}
mlr<- collect_metrics(tune_res) %>% 
          arrange(mean)
mlr
mlr_auc<- max(mlr$mean)
mlr_auc  
```

I got the 0.8145077 accurancy with this model.


### Model selecting

I've been using 4 models with each values of mean(auc). Now I need to decide which one has better mean and will be best model.

```{r}

table <- matrix(c(best_auc, random_auc, boost_auc,mlr_auc  ),ncol =4)
rownames(table) <- c('roc auc')
colnames(table) <- c('best-performing pruned tree', 'randomforest','boosted tree models',' Multinomial logistic regression')
table

```

So the random forest is the best performed model.


```{r}
best_model <- select_best(rf_tune, metric = 'roc_auc')
final1<- finalize_workflow(rf_workflow, best_model)
final_fit1<- fit(final1, train)
final_fit1

```

I applied the test data set to the random forest model and predict the possible percentage of outcome of 0 and 1. And this lead me to get the predicted outcome. 

```{r}
predicted_data <- augment(final_fit1, new_data = test) %>% 
  select(outcome, starts_with(".pred"))
predicted_data 
```
I got the accuracy of this model.

```{r}
predict(final_fit1, new_data = test, type = "class") %>% 
  bind_cols(test %>% select(outcome)) %>% 
  accuracy(truth = outcome, estimate = .pred_class)
```

81.2 % correct with the test data set. I would say the model selecting was good. 


However, in my opinion, I want to use the Multinomial logistic regression model again because it had only 0.001 difference.


```{r}
second_best_model <- select_best(tune_res, metric = 'roc_auc')
final2<- finalize_workflow(en_workflow, second_best_model)
final_fit2<- fit(final2, train)
```


```{r}
predicted_data <- augment(final_fit2, new_data = test) %>% 
  select(outcome, starts_with(".pred"))
predicted_data$.pred_class
```
I applied this model to test data set.

```{r}
predict(final_fit2, new_data = test, type = "class") %>% 
  bind_cols(test %>% select(outcome)) %>% 
  accuracy(truth = outcome, estimate = .pred_class)

plot(1:154, predicted_data$.pred_class)
plot(1:154, test$outcome) 


```

It is Interesting. This model has better accuracy.



### Conclusion

I was able to fit 4 models and I picked the random forest model(RF) as the best performed model. And I pick the MLR as second best performed model. The RF model had 0.8181818 of accuracy. The MLR model had 0.8311688	of accuracy. Even if my data set was not bit, I was able to get good models to predict what factors will affect to the diabetes diagnosis and how will affect to it. Because the plots I created, I was able to notice what variables was useful or not.  